{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pi_target = np.load('taxi-policy/pi0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19813083098352882"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_target[100,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.Tensor(pi_target)[1,:].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3923, 0.3902, 0.3489, 0.3095, 0.2964, 0.2628],\n",
       "        [0.2931, 0.3236, 0.3794, 0.3221, 0.3093, 0.3725],\n",
       "        [0.2994, 0.3482, 0.3041, 0.3989, 0.2720, 0.3773],\n",
       "        ...,\n",
       "        [0.3746, 0.2684, 0.3871, 0.3750, 0.3202, 0.2747],\n",
       "        [0.3637, 0.2935, 0.3135, 0.3334, 0.2985, 0.3974],\n",
       "        [0.3378, 0.3300, 0.2922, 0.2835, 0.4250, 0.3316]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*torch.Tensor(pi_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    alpha = 0.6\n",
    "    pi_eval = torch.Tensor(np.load('taxi-policy/pi19.npy'))\n",
    "    pi_behavior = torch.Tensor(np.load('taxi-policy/pi3.npy'))\n",
    "    pi_behavior = alpha * pi_eval + (1-alpha) * pi_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_behavior[1,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(state_num, env, policy, num_trajectory, truncate_size):\n",
    "    SASR = []\n",
    "    total_reward = 0.0\n",
    "    frequency = np.zeros(state_num)\n",
    "    for i_trajectory in range(num_trajectory):\n",
    "        state = env.reset()\n",
    "        sasr = []\n",
    "        for i_t in range(truncate_size):\n",
    "#             env.render()\n",
    "            p_action = policy[state, :]\n",
    "            action = np.random.choice(p_action.shape[0], 1, p = p_action)[0]\n",
    "            next_state, reward = env.step(action)\n",
    "\n",
    "            sasr.append((state, action, next_state, reward))\n",
    "            frequency[state] += 1\n",
    "            total_reward += reward\n",
    "            #print env.state_decoding(state)\n",
    "            #a = input()\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "        SASR.append(sasr)\n",
    "    return SASR, frequency, total_reward/(num_trajectory * truncate_size) #SASR ntxts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from environment import taxi\n",
    "import numpy as np\n",
    "length = 5\n",
    "env = taxi(length)\n",
    "n_state = env.n_state\n",
    "n_action = env.n_action\n",
    "print_freq=20\n",
    "\n",
    "\n",
    "alpha = np.float(0.6)\n",
    "pi_eval = np.load('taxi-policy/pi19.npy')\n",
    "pi_behavior = np.load('taxi-policy/pi3.npy')\n",
    "pi_behavior = alpha * pi_eval + (1-alpha) * pi_behavior\n",
    "# roll_out(n_state, env, pi_behavior, 1, 100)\n",
    "SASR_b, b_freq, _ = roll_out(n_state, env, pi_behavior, 1, 10000000)  #1, 1000000\n",
    "SASR_e, e_freq, _ = roll_out(n_state, env, pi_eval, 1, 10000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(b_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pi3_distrib.npy', b_freq/np.sum(b_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pi19_distrib.npy', e_freq/np.sum(e_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_freq_distrib = b_freq/np.sum(b_freq)\n",
    "e_freq_distrib = e_freq/np.sum(e_freq)\n",
    "gt_w = e_freq_distrib/b_freq_distrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3336], device='cuda:0', grad_fn=<SelectBackward>) 0.5625\n",
      "tensor([0.9280], device='cuda:0', grad_fn=<SelectBackward>) 1.1931137724550898\n",
      "tensor([-0.8135], device='cuda:0', grad_fn=<SelectBackward>) 1.1825503355704696\n",
      "tensor([-0.1770], device='cuda:0', grad_fn=<SelectBackward>) 1.3074866310160427\n",
      "tensor([-0.5757], device='cuda:0', grad_fn=<SelectBackward>) 1.1855828220858895\n",
      "tensor([-0.8149], device='cuda:0', grad_fn=<SelectBackward>) 2.221052631578947\n",
      "tensor([-0.5776], device='cuda:0', grad_fn=<SelectBackward>) 1.0903490759753593\n",
      "tensor([0.4172], device='cuda:0', grad_fn=<SelectBackward>) 1.0702947845804989\n",
      "tensor([-0.6429], device='cuda:0', grad_fn=<SelectBackward>) 1.1491525423728812\n",
      "tensor([1.1484], device='cuda:0', grad_fn=<SelectBackward>) 1.1859939759036144\n",
      "tensor([0.5444], device='cuda:0', grad_fn=<SelectBackward>) 1.5\n",
      "tensor([0.6959], device='cuda:0', grad_fn=<SelectBackward>) 0.970059880239521\n",
      "tensor([-0.9281], device='cuda:0', grad_fn=<SelectBackward>) 1.0180722891566265\n",
      "tensor([-0.2968], device='cuda:0', grad_fn=<SelectBackward>) 1.1098265895953758\n",
      "tensor([-1.0018], device='cuda:0', grad_fn=<SelectBackward>) 1.1360294117647058\n",
      "tensor([-0.4231], device='cuda:0', grad_fn=<SelectBackward>) 1.1515151515151514\n",
      "tensor([-0.6137], device='cuda:0', grad_fn=<SelectBackward>) 1.125\n",
      "tensor([1.7876], device='cuda:0', grad_fn=<SelectBackward>) 0.9816849816849816\n",
      "tensor([1.7247], device='cuda:0', grad_fn=<SelectBackward>) 1.3562753036437247\n",
      "tensor([0.7065], device='cuda:0', grad_fn=<SelectBackward>) 1.0630407911001236\n",
      "tensor([-0.9182], device='cuda:0', grad_fn=<SelectBackward>) 0.9565217391304348\n",
      "tensor([-0.0963], device='cuda:0', grad_fn=<SelectBackward>) 1.2139737991266375\n",
      "tensor([1.1651], device='cuda:0', grad_fn=<SelectBackward>) 1.095808383233533\n",
      "tensor([0.7648], device='cuda:0', grad_fn=<SelectBackward>) 1.2160228898426324\n",
      "tensor([0.5027], device='cuda:0', grad_fn=<SelectBackward>) 1.1776\n",
      "tensor([0.2598], device='cuda:0', grad_fn=<SelectBackward>) 1.3972602739726028\n",
      "tensor([2.8535], device='cuda:0', grad_fn=<SelectBackward>) 0.8988095238095238\n",
      "tensor([-0.5719], device='cuda:0', grad_fn=<SelectBackward>) 0.9254385964912281\n",
      "tensor([0.7159], device='cuda:0', grad_fn=<SelectBackward>) 1.0813308687615526\n",
      "tensor([-0.8307], device='cuda:0', grad_fn=<SelectBackward>) 1.1649040344692518\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from DeepGMM_w import StateEmbedding\n",
    "w = StateEmbedding().cuda()\n",
    "history=torch.load(open('240.pth','rb'))\n",
    "w.load_state_dict(history['w_model'])\n",
    "s=torch.linspace(0, 1999, steps=2000).cuda().long()\n",
    "for i in range(30):   \n",
    "    print(w(s)[i],gt_w[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3336,  0.9280, -0.8135, -0.1770, -0.5757, -0.8149, -0.5776,  0.4172,\n",
      "        -0.6429,  1.1484], device='cuda:0', grad_fn=<AsStridedBackward>) \n",
      " [0.5625     1.19311377 1.18255034 1.30748663 1.18558282 2.22105263\n",
      " 1.09034908 1.07029478 1.14915254 1.18599398]\n"
     ]
    }
   ],
   "source": [
    "print(w(s)[:10].flatten(),'\\n',gt_w[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy(SASR, gamma):\n",
    "    total_reward = 0.0\n",
    "    discounted_t = 1.0\n",
    "    self_normalizer = 0.0\n",
    "    for (state, action, next_state, reward) in SASR:\n",
    "        total_reward += reward * discounted_t\n",
    "        self_normalizer += discounted_t\n",
    "        discounted_t *= gamma\n",
    "    return total_reward/self_normalizer #why take the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1f196f1f3b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mSASR_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroll_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pi_e doesn't need loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgt_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mon_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSASR_e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(state_num, env, policy, num_trajectory, truncate_size):\n",
    "    SASR = []\n",
    "    total_reward = 0.0\n",
    "    frequency = np.zeros(state_num)\n",
    "    for i_trajectory in range(num_trajectory):\n",
    "        state = env.reset()\n",
    "        for i_t in range(truncate_size):\n",
    "            #env.render()\n",
    "            p_action = policy[state, :]\n",
    "            action = np.random.choice(p_action.shape[0], 1, p = p_action)[0]\n",
    "            next_state, reward = env.step(action)\n",
    "\n",
    "            SASR.append((state, action, next_state, reward))\n",
    "            frequency[state] += 1\n",
    "            total_reward += reward\n",
    "            #print env.state_decoding(state)\n",
    "            #a = input()\n",
    "            state = next_state\n",
    "    return SASR, frequency, total_reward/(num_trajectory * truncate_size) #SASR ntxts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(300)\n",
    "np.random.seed(300)\n",
    "torch.manual_seed(300)\n",
    "pi_eval = np.load('taxi-policy/pi19.npy')\n",
    "n_state = env.n_state\n",
    "SASR_e, _, _ = roll_out(n_state, env, pi_eval, 200, 400) #pi_e doesn't need loader\n",
    "# gt_reward = on_policy(np.array(SASR_e), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1495"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=[]\n",
    "for sasr in SASR_e:\n",
    "    acc.append(torch.Tensor(sasr)[:,-1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14950001"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eta(pi_eval, pi_behavior, n_state, n_action):\n",
    "    #eq(6) in Liu's paper\n",
    "    eta=torch.zeros((n_state, n_action))\n",
    "    for state in range(n_state):\n",
    "        for action in range(n_action):\n",
    "            #Due to Markov property, given the state the quality of a certain action is always the same.\n",
    "            eta[state, action] = pi_eval[state, action]/pi_behavior[state, action]  \n",
    "    return eta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = get_eta(pi_eval, pi_behavior, n_state, n_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(200)\n",
    "np.random.seed(200)\n",
    "torch.manual_seed(200)\n",
    "SASR_b_val_raw, _, _ = roll_out(n_state, env, pi_behavior,5,10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "SASR_b_val=torch.Tensor(SASR_b_val_raw)\n",
    "s = SASR_b_val[:,0].long()\n",
    "a = SASR_b_val[:,1].long()\n",
    "r = SASR_b_val[:,-1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1239)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gt_w[s].squeeze() * eta[s,a] * r).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_w=torch.Tensor(gt_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6230)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_w[884]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 884, 1329, 1249,  ...,   61,  461,  461]),\n",
       " tensor([0., 3., 2.,  ..., 0., 4., 0.]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3270)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta[1329,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1, -1, -1,  ..., -1, -1, -1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.1167"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
